install.packages("randomForest")
PLdata <- read.csv(file.choose(), header = TRUE)
attach(PLdata)
summary(PLdata)
library(randomForest)
pl.forest <- randomForest(FTR ~., data = PLdata)
print(PLdata[,c("FTR")])
summary(PLdata[,c("FTR")])
na.exclude(PLdata)
pl.forest <- randomForest(FTR ~., data = na.roughfix(PLdata))
View(PLdata)
View(PLdata)
HomeTeam <- as.array(HomeTeam)
rm(HomeTeam)
class(HomeTeam)
toString(HomeTeam)
HomeTeam[] <- lapply(HomeTeam, as.character)
class(HomeTeam)
View(PLdata)
rm(HomeTeam)
pl.forest <- randomForest(FTR ~., data = PLdata)
pl.forest <- randomForest(FTR ~., data = na.roughFix(Pldata))
pl.forest <- randomForest(FTR ~., data = na.roughfix(Pldata))
pl.forest <- randomForest(FTR ~., data = na.roughfix(PLdata))
as.numeric(as.character(PLdata$HomeTeam))
as.character(PLdata$HomeTeam)
class(HomeTeam)
install.packages("naiveBayes")
install.packages("naivebayes")
library(naivebayes)
attach(PLdata)
PLNaiveBayesModel <- naiveBayes(FTR~., data = PLdata)
library(e1071)
PLNaiveBayesModel <- naiveBayes(FTR~., data = PLdata)
View(PLNaiveBayesModel)
PLTesting <- read.csv(file.choose(), header = TRUE)
PLTesting <- PLTesting[,-3]
PLPredictions(PLNaiveBayesModel, PLTesting)
PLPredictions <- predict(PLNaiveBayesModel, PLTesting)
print(PLPredictions)
PLMasterData <- PLdata
rm(Pldata)
rm(PLdata)
PLTrainingData <- read.csv(file.choose(), header = TRUE)
PLMasterData <- PLdata
PLTrainingData <- read.csv(file.choose(), header = TRUE)
PLTesting <- read.csv(file.choose(), header = TRUE)
PLTesting <- PLTesting[,-3]
PLMasterData <- read.csv(file.choose(), header = TRUE)
PLTrainingData <- read.csv(file.choose(), header = TRUE)
PLTesting <- read.csv(file.choose(), header = TRUE)
PLTesting <- PLTesting[,-3]
PLTestingData <- read.csv(file.choose(), header = TRUE)
PLTestingData <- PLTestingData[,-3]
rm(PLTesting)
nrows(PLTrainingData)
nrow(PLTrainingData)
PLTrainingData <- read.csv(file.choose(), header = TRUE)
PLNaiveBayesModel <- naiveBayes(FTR~., data = PLTrainingData)
library(e1071)
PLNaiveBayesModel <- naiveBayes(FTR~., data = PLTrainingData)
PLPredictions <- predict(PLNaiveBayesModel, PLTesting)
PLPredictions <- predict(PLNaiveBayesModel, PLTestingData)
print(PLPredictions)
print(PLMasterData[6841-6861,"FTR"])
print(PLMasterData[6841:6861,"FTR"])
print(PLMasterData[6841:6860,"FTR"])
print(PLPredictions)
print(PLMasterData[6841:6860,"FTR"])
class(PLPredictions)
PLPredictions[2]
for(i in 1:20){
print(PLPredictions[i] == PLMasterData[6840+i])
}
for(i in 1:20){
print(PLPredictions[i] == PLMasterData[6840+i,])
}
print(PLPredictions[i] == PLMasterData[6840+i,FTR])
}
for(i in 1:20){
print(PLPredictions[i] == PLMasterData[6840+i,FTR])
}
for(i in 1:20){
print(PLMasterData[6840+i,FTR])
}
for(i in 1:20){
print(PLMasterData[c(6840+i),FTR])
}
PLLast20 <- PLMasterData[6841:6860,"FTR"]
PLLast20 <- PLMasterData[6841:6860,"FTR"]
PLPredictions <- as.vector(PLPredictions)
PLPredictions
PLLast20
PLLast20 <- as.character(PLLast20)
PLLast20
for(i in 1:length(PLLast20)){
print(PLLast20[i]==PLPredictions[i])
}
PredictionResults <- PLLast20 == PLPredictions
class(PredictionResults)
summary(PredictionResults)
PLNaiveBayesModel <- naiveBayes(FTR~., data = PLTrainingData)
PLNaiveBayesModel <- naiveBayes(FTR~., data = PLTrainingData)
PLPredictions <- predict(PLNaiveBayesModel, PLTestingData)
print(PLPredictions)
PLLast20 <- PLMasterData[6841:6860,"FTR"]
PLPredictions <- as.vector(PLPredictions)
class(PredictionResults)
summary(PredictionResults)
library(rsample)      # data splitting
library(gbm)          # basic implementation
library(xgboost)      # a faster implementation of gbm
library(caret)        # an aggregator package for performing many machine learning models
library(h2o)          # a java-based platform
library(pdp)          # model visualization
library(ggplot2)      # model visualization
library(lime)         # model visualization
install.packages("rsample")
install.packages("gbm")
install.packages("xgboost")
install.packages("caret")
install.packages("caret")
install.packages("h20")
install.packages("h2o")
install.packages("pdp")
install.packages("ggplot2")
install.packages("lime")
library(rsample)      # data splitting
library(gbm)          # basic implementation
library(xgboost)      # a faster implementation of gbm
library(caret)        # an aggregator package for performing many machine learning models
library(h2o)          # a java-based platform
library(pdp)          # model visualization
library(ggplot2)      # model visualization
library(lime)         # model visualization
gbm.fit <- gbm(
formula = FTR ~ .,
distribution = "gaussian",
data = PLTrainingData,
n.trees = 10000,
interaction.depth = 1,
shrinkage = 0.001,
cv.folds = 5,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
print(gbm.fit)
View(PLTestingData)
View(PLTestingData)
print(PLPredictions)
print(PLPredictions)
print(PLLast20)
PLLast20 <- as.character(PLMasterData[6841:6860,"FTR"])
PLPredictions <- as.vector(PLPredictions)
print(PLPredictions)
print(PLLast20)
library(e1071)
PLPredictionResults <- PLLast20 == PLPredictions
print(PLPredictionResults)
summary(PLPredictionResults)
print(gbm.fit)
PLGBMPredictions <- predict(gbm.fit, n.trees = gbm.fit$n.trees, PLTestingData)
attach(PLTrainingData)
attach(PLTestingData)
PLGBMPredictions <- predict(gbm.fit, n.trees = gbm.fit$n.trees, PLTestingData)
print(PLTrainingData[,c(2,3)])
PLGBMPredictions <- predict(gbm.fit, n.trees = gbm.fit$n.trees, PLTestingData)
gbm.fit <- gbm(
formula = FTR~.,
distribution = "gaussian",
data = PLTrainingData,
n.trees = 10000,
interaction.depth = 1,
shrinkage = 0.001,
cv.folds = 5,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
PLGBMPredictions <- predict(gbm.fit, n.trees = gbm.fit$n.trees, PLTestingData)
View(PLMasterData)
View(PLMasterData)
PLNaiveBayesModel <- naiveBayes(FTR~., data = PLTrainingData[,c(1,2,5)])
View(PLTestingData)
PLPredictions <- predict(PLNaiveBayesModel, PLTestingData)
PLLast20 <- as.character(PLMasterData[6841:6860,"FTR"])
PLPredictions <- as.vector(PLPredictions)
print(PLPredictions)
print(PLLast20)
PLPredictionResults <- PLLast20 == PLPredictions
print(PLPredictionResults)
summary(PLPredictionResults)
gbm.fit <- gbm(
formula = FTR~HomeTeam+AwayTeam,
distribution = "gaussian",
data = PLTrainingData,
n.trees = 10000,
interaction.depth = 1,
shrinkage = 0.001,
cv.folds = 5,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
gbm.fit <- gbm(
formula = FTR~.,
distribution = "gaussian",
data = PLTrainingData[,c(1,2,5)],
n.trees = 10000,
interaction.depth = 1,
shrinkage = 0.001,
cv.folds = 5,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
PLGBMPredictions <- predict(gbm.fit, n.trees = gbm.fit$n.trees, PLTestingData)
print(PLGBMPredictions)
PLGBMPredictions <- predict(gbm.fit, n.trees = gbm.fit$n.trees, PLTestingData, type="response")
print(PLGBMPredictions)
PLGBMPredictions <- predict(object = gbm.fit,
newdata = PLTestingData,
n.trees =gbm.fit$n.trees,
type = "response")
print(PLGBMPredictions)
print(PLGBMPredictions)
PLTrainingData
PLTrainingData <- PLTrainingData[,c(1,2,5)]
PLTrainingData <- PLTrainingData[,c(1,2,5)]
library(randomForest)
PLForest <- randomForest(FTR~., data = PLTrainingData)
View(PLForest)
PLGBMPredictions <- predict(object = gbm.fit,
newdata = PLTestingData,
n.trees =gbm.fit$n.trees,
type = "votes")
print(gbm.fit)
gbm.fit <- gbm(
formula = FTR~.,
distribution = "gaussian",
data = PLTrainingData[,c(1,2,5)],
n.trees = 10000,
interaction.depth = 1,
shrinkage = 0.001,
cv.folds = 5,
n.cores = NULL, # will use all cores by default
verbose = FALSE
)
library(rsample)      # data splitting
library(gbm)          # basic implementation
library(xgboost)      # a faster implementation of gbm
library(caret)        # an aggregator package for performing many machine learning models
library(h2o)          # a java-based platform
library(pdp)          # model visualization
library(ggplot2)      # model visualization
library(lime)         # model visualization
PLGBMPredictions <- predict(object = gbm.fit,
newdata = PLTestingData,
n.trees =gbm.fit$n.trees,
type = "votes")
PLForestPredictions <- predict(PLForest, newdata = PLTestingData)
PLForestPredictions <- predict(object = PLForest, newdata = PLTestingData)
XGModel <- xgboost(data = PLTrainingData, max.depth = 2, eta = 1, nthread = 2, nrounds = 2)
PLTrainingMatrix <- as.matrix(PLTrainingData)
XGModel <- xgboost(data = PLTrainingMatrix, max.depth = 2, eta = 1, nthread = 2, nrounds = 2)
data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')
train <- agaricus.train
test <- agaricus.test
print(train$label)
print(train)
class(train)
library(randomForest)
PLForestPredictions <- predict(object = PLForest, newdata = PLTestingData)
SparseMatrix <- sparse.model.matrix(FTR~.-1, data = PLTrainingData)
install.packages("Matrix")
install.packages("Matrix")
install.packages("Matrix")
install.packages("Matrix")
install.packages("Matrix")
install.packages("Matrix")
library(Matrix)
SparseMatrix <- sparse.model.matrix(FTR~.-1, data = PLTrainingData)
labels = PLTrainingData['labels']
PLTrainingData = PLTrainingData[-grep('labels', colnames(PLTrainingData))]
PLGBMPredictions <- predict(object = gbm.fit,
newdata = PLTestingData,
n.trees =gbm.fit$n.trees,
type = "link")
library(rsample)      # data splitting
library(gbm)          # basic implementation
library(xgboost)      # a faster implementation of gbm
library(caret)        # an aggregator package for performing many machine learning models
library(h2o)          # a java-based platform
library(pdp)          # model visualization
library(ggplot2)      # model visualization
library(lime)         # model visualization
library(Matrix)
PLGBMPredictions <- predict(object = gbm.fit,
newdata = PLTestingData,
n.trees =gbm.fit$n.trees,
type = "link")
print(PLGBMPredictions)
View(gbm.fit)
View(SparseMatrix)
